# ğŸ“Š Forex Data Pipeline: Multi-source ETL

Un pipeline de donnÃ©es complet pour collecter, transformer et stocker les taux de change Ã  partir de trois sources diffÃ©rentes :  
- ğŸ’¾ Un fichier CSV historique (Kaggle)  
- ğŸŒ L'API [Frankfurter.app](https://www.frankfurter.app)  
- ğŸ•·ï¸ Web scraping en direct depuis [x-rates.com](https://www.x-rates.com)  

Toutes les donnÃ©es sont traitÃ©es et stockÃ©es dans une base de donnÃ©es **SQLite**, ainsi que dans des fichiers CSV organisÃ©s.

---

## ğŸ“ Structure du projet

```
forex-data-pipeline/
â”‚
â”œâ”€â”€ main.py                    # Script principal qui lance tous les ETL
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ csv_loader.py          # Extraction + traitement du fichier CSV local (2 mois)
â”‚   â”œâ”€â”€ api_fetcher.py         # Extraction des donnÃ©es depuis l'API Frankfurter
â”‚   â”œâ”€â”€ web_scraper.py         # Scraping en direct depuis x-rates.com
â”‚   â”œâ”€â”€ supabase_uploader.py   # Envoi des nouvelles donnÃ©es vers Supabase
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ supabase.py   # Envoi des nouvelles donnÃ©es vers Supabase
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Contient les donnÃ©es CSV brutes (ex: de Kaggle)
â”‚   â””â”€â”€ processed/             # Contient les CSV traitÃ©s par chaque source (gÃ©nÃ©rÃ© automatiquemant)
â”‚
â”œâ”€â”€ database/
â”‚   â””â”€â”€ forex_data.db          # Base de donnÃ©es SQLite contenant toutes les donnÃ©es
â”‚
â”œâ”€â”€ .github/workflows/         # Configuration GitHub Actions pour exÃ©cutions automatiques
â”œâ”€â”€ .env                       # Contient les variables sensibles (email, Supabase, etc.)
â”œâ”€â”€ .gitignore                 # Fichiers Ã  ignorer par Git
â””â”€â”€ README.md                  # Ce fichier
```

---

## âš™ï¸ FonctionnalitÃ©s principales

- âœ… **Orchestration** complÃ¨te via main.py
- âœ… **Sauvegarde doubl**e (CSV + SQLite)
- âœ… **Nettoyage et transformation** des donnÃ©es
- âœ… **Traitement intelligent du CSV Kaggle** : extraction uniquement des 2 derniers mois par defaut
- âœ… **Web scraping dynamique** avec alerte email si structure HTML change
- âœ… **Synchronisation vers Supabase** des donnÃ©es rÃ©centes avec alerte email en cas d'Ã©chec
- âœ… **Logging clair** dans tous les scripts
- âœ… **Automatisation du pipeline** via GitHub Action

---

## ğŸ” Automatisation avec GitHub Actions

Le pipeline sâ€™exÃ©cute automatiquement chaque jour grÃ¢ce Ã  GitHub Actions :

- â±ï¸ Planification quotidienne via cron
- ğŸ”„ Lancement automatique des trois ETL (CSV, API, scraping)
- ğŸ“¤ Upload des nouvelles donnÃ©es vers Supabase
- ğŸ§ª Ã€ terme : ajout de tests automatiques pour garantir la qualitÃ© des donnÃ©es
- âš™ï¸ Le tout est gÃ©rÃ© dans le fichier `.github/workflows/etl.yml`

![Automatic Pipeline Sucessfuly excecuted daily](https://github.com/user-attachments/assets/2f303689-457b-492e-ab85-d88dbd268c3d)

---

## ğŸ”” Monitoring et alertes

Si le scraping Ã©choue (ex: structure HTML modifiÃ©e), une fonction `alert_admin()` envoie automatiquement un **email dâ€™alerte**.

> ğŸ›¡ï¸ Les identifiants sont stockÃ©s en toute sÃ©curitÃ© dans `.env`.

![screenshot - Email received when there was error in excecution](https://github.com/user-attachments/assets/094a571e-abb2-4a10-be9c-8eddd6f96911)

---

## ğŸš€ Lancer le projet

### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/ton-username/forex-data-pipeline.git
cd forex-data-pipeline
```

### 2. CrÃ©er un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate   # ou .\venv\Scripts\activate sur Windows
pip install -r requirements.txt
```

### 3. Configurer `.env`

CrÃ©er un fichier `.env` Ã  la racine :

```
SMTP_SERVER = "smtp.gmail.com"  # Pour Gmail (ou utilises ton provider SMTP)
SMTP_PORT = 587 # Le port
EMAIL_ADDRESS = "ton.nom@email.com"
EMAIL_PASSWORD = "tonMotDePasse" # Pour Gmail, tu peux crÃ©er un "mot de passe d'application". Visites https://myaccount.google.com/apppasswords
RECIPIENT_EMAIL = "admin.nom@email.com"

# Supabase credentials
SUPABASE_URL = "https://your-project.supabase.co"
SUPABASE_KEY = "your-secret-key"
```

### 4. Lancer le pipeline
```bash
python main.py
```

---

## ğŸ“¦ DÃ©pendances principales

- `requests`
- `pandas`
- `beautifulsoup4`
- `python-dotenv`
- `sqlite3`
- `pytz`
- `python-dotenv`
- `smtplib` (standard lib)
- `supabase-py`

---

## ğŸ§  Prochaines Ã©volutions

- [ ] Dashboard simple avec GitHub Pages
- [ ] Tests unitaires avec `pytest`
- [ ] TÃ©lÃ©chargement automatique du CSV Kaggle via API

---

## ğŸ§‘â€ğŸ’» Auteur

**Gervais Yao Amoah** â€“ *Projet personnel Data Engineering*  
[GitHub](https://github.com/gervais-amoah) | [LinkedIn](https://linkedin.com/in/gervais-amoah)

---

## ğŸ“ Licence

MIT â€“ libre Ã  utiliser, amÃ©liorer et partager.
